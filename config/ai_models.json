{
  "mixtral": {
    "name": "mixtral:latest",
    "endpoint": "/api/generate",
    "temperature": 0.7,
    "max_tokens": 2048,
    "context_length": 4096
  },
  "llama3.2": {
    "name": "llama3.2:3b",
    "endpoint": "/api/generate",
    "temperature": 0.8,
    "max_tokens": 1024,
    "context_length": 2048
  },
  "codellama": {
    "name": "codellama:latest",
    "endpoint": "/api/generate",
    "temperature": 0.3,
    "max_tokens": 4096,
    "context_length": 4096
  }
} 